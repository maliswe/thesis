\section{Related Work}

\subsection{Impact of AI on Developer Code Writing Speed}
AI-powered coding assistants have been widely adopted, with studies suggesting significant productivity benefits. Microsoft Research found that developers using GitHub Copilot completed tasks 55.8\% faster than those coding manually \cite{peng2023impact}. IBM’s study on AI pair programming in an enterprise setting observed perceived productivity gains but noted variability based on task complexity and developer experience \cite{weisz2025examining}.

\subsection{Security Risks in AI-Generated Code}
Security concerns in AI-generated code have been extensively documented. One of the first empirical evaluations of GitHub Copilot revealed that approximately 40\% of AI-generated code contained vulnerabilities, including SQL injection and buffer overflow issues \cite{fu2025security}. Perry et al. found that AI-assisted developers wrote significantly more insecure code than those without AI assistance, despite being more confident in their solutions \cite{perry2023}. Additionally, Asare et al. demonstrated that AI-generated code often replicated past vulnerabilities, repeating the same mistakes found in historical software security incidents \cite{asare2024}. 

These findings suggest that while AI tools accelerate development, they also introduce new security risks that must be mitigated. After these studies were conducted, a study by Peslak and Kovalchick in 2024 analyzed data from the Stack Overflow Annual Developer Survey to assess the usage patterns of ChatGPT and GitHub Copilot among programmers. The findings revealed that 58.8\% of respondents regularly used ChatGPT, while 24.8\% utilized GitHub Copilot \cite{peslak2024ai}. This confirms that a significant number of developers use AI tools to generate code, even when evidence indicates that AI-generated code contains security vulnerabilities and risks.

\subsection{Code Reuse and Risks in Stack Overflow}
Prior to the rise of AI coding assistants, researchers identified similar patterns of developer behavior on community platforms like Stack Overflow. Acar et al. \cite{acar2016stackoverflow} found that developers often copy security-sensitive code from Stack Overflow without understanding its implications or verifying its safety. In many cases, insecure cryptographic code was integrated into applications due to its high visibility and upvotes.

Yang et al. \cite{yang2017code} empirically demonstrated that vulnerable code snippets are frequently copied from Stack Overflow into open-source projects on GitHub, often without modification or proper attribution. These findings indicate that the trade-off between development convenience and code security predates AI but is likely exacerbated by the rapid and automated nature of AI-generated suggestions.

This parallel reinforces that the core challenge lies not just in the technology used, but in developer behavior—specifically, the tendency to prioritize speed and perceived reliability over careful validation. Understanding this behavioral continuity can help frame AI tools as a new chapter in an ongoing issue, rather than an entirely novel risk.


\subsection{Developer Adoption and Perceptions}
Despite security risks, the adoption of AI coding assistants has grown rapidly. A 2023 survey by GitHub reported that over 70\% of developers believed Copilot helped them stay focused and avoid mental fatigue \cite{githubSurvey}. However, Snyk’s industry report found that while 75\% of developers believed AI-generated code was more secure than human-written code, 56\% of respondents admitted to encountering security issues in AI-generated suggestions \cite{snyk2023}.

Additionally, empirical research further emphasizes this security concern. A study evaluating the security of GitHub Copilot’s code contributions found that approximately 40\% of generated code contained security vulnerabilities, including common weaknesses such as SQL injection and improper input validation. The likelihood of generating vulnerable code was influenced by the context and specificity of the prompts provided to the AI tool, as well as the type of programming language and the domain of the coding task \cite{pearce2021asleep}.

These findings collectively indicate a potential overconfidence bias in AI-assisted development, where developers may unknowingly trust insecure AI-generated code, underlining the critical importance of careful review and validation practices.


\subsection{Challenges of AI-Generated Code Security}
Before diving into verification techniques, it’s important to recognize why AI-generated code demands special attention:
\begin{itemize}
    \item \textbf{Propagation of Insecure Patterns:} AI coding models are trained on massive datasets of existing code, which likely include insecure coding patterns. Thus, they can inadvertently reproduce known vulnerabilities present in training data \cite{fu2025security}. In practice, this means an AI might suggest a snippet that “looks” correct but introduces a security flaw (e.g., using a deprecated cryptographic function or mishandling user input).
    \item \textbf{False Sense of Security:} Developers may overtrust AI-generated code. An academic user study by Perry et al. (2023) found that participants using an AI assistant wrote significantly less secure code than those coding manually, yet they were more confident that their code was secure \cite{perry2023}. In other words, the convenience of AI suggestions can lull developers into assuming the output is safe, reducing their vigilance in reviewing it. This overconfidence highlights why explicit verification steps are crucial.
    \item \textbf{Speed vs. Security Review:} AI can produce code much faster than humans. Industry reports note that the speed of AI-generated code can outpace the ability of security teams to audit and validate it \cite{scworld2024ai}. . When code is generated in seconds, the usual code review and testing processes might be rushed or skipped, leading to vulnerabilities slipping through. A 2023 survey of security leaders found 66\% believe it’s impossible for security teams to keep up with AI-powered development using current practices \cite{cyberark2024press}
    \item \textbf{Incomplete Context:} AI tools often generate code in isolation (e.g., a single function or snippet). Such code might be incomplete or lack context (like missing import statements, framework setup, or surrounding logic). Traditional verification tools (linters, static analyzers) may struggle with these partial snippets, as they expect complete code. This necessitates either providing additional context or using specialized tools that can evaluate incomplete code segments \cite{deluca2024devaic}
\end{itemize}
Given these challenges, a combination of verification practices is emerging to ensure AI-generated code doesn’t compromise web application security. Below, we outline the existing practices – from manual reviews to automated analysis – and summarize what research says about their effectiveness.

\subsection{Existing Security Verification Practices for AI-Generated Code}
\begin{enumerate}
    \item \textbf{Manual Code Review and Testing} Human review remains a fundamental line of defense for any code, including AI-generated snippets. Organizations leveraging AI code assistants are strongly advised to treat AI output as potentially insecure by default and subject it to the same (or greater) scrutiny as human-written code. Industry experts emphasize this repeatedly. For example, Trend Micro’s security analysis of ChatGPT-generated code urges developers to “treat all code generated by ChatGPT as if it contains vulnerabilities” and to thoroughly test and review it before deploying \cite{trendmicro}. Similarly, Synopsys' whitepaper notes that AI-generated code can significantly speed up development, but you still need to review and verify it as much, if not more, than code written by your developers \cite{blackduck}.
    \item \textbf{Static Analysis and SAST Tools} Static Application Security Testing (SAST) tools automatically scan source code for known vulnerability patterns and poor practices. These include linters, security scanners, and even AI-powered code review tools. For AI-generated code, static analysis is one of the first and most straightforward verification steps to apply: once the code is generated (and integrated into a codebase), run scanners to flag issues. Industry guidance strongly supports this. Synopsys (Black Duck) recommends that organizations implement rigorous verification processes, including static analysis, to validate the security and quality of AI-generated code \cite{blackduck}. . In practice, this might involve integrating tools like SonarQube, ESLint (with security plugins), PyLint, Checkmarx, Snyk Code, or GitHub CodeQL into the development pipeline. These tools can catch common issues such as SQL injection risks (e.g., use of string concatenation in DB queries), cross-site scripting (unsanitized output in HTML), hard-coded credentials, etc.
    \item \textbf{Dynamic Testing and Fuzzing} Dynamic analysis involves testing the running application (or running code) to find security issues that manifest only at runtime. This includes techniques like fuzz testing (supplying a wide range of random or crafted inputs to see if the code breaks or behaves unexpectedly), integration tests, and full application scans (like DAST tools that simulate attacker behavior against a web app). For AI-generated code, dynamic testing is a way to verify that code actually performs securely in context. For example, if an AI generates an input validation function, fuzzing that function with malicious inputs can confirm whether it truly sanitizes or not. If an AI generates a piece of authentication logic, dynamic tests can check if unauthorized access is properly prevented. While fewer academic papers focus on dynamic testing specifically for AI code (most focus on static vulnerability presence), industry experts imply its importance. The need for thorough testing is often stressed in guidance for AI code usage \cite{dynamictesting}. In fact, the research community uses dynamic testing as a primary measure of success for AI code generation models. Typically, these models are evaluated on functional correctness by checking how often the generated code passes a given test suite \cite{dynamictestingchatgpt}. For instance, OpenAI’s own evaluation of the Codex model (which powers Copilot) used a benchmark called HumanEval, consisting of coding problems each with unit tests. Codex was asked to generate solutions, and those solutions were considered correct only if they passed all the test cases. In the original results, Codex could solve 28.8\% of the problems with its first attempt (by comparison, a baseline GPT-3 solved 0\%) \cite{evaluatingChatgpt}. The fact that over 70\% of initial generations failed at least one test illustrates that even advanced code models frequently produce incorrect output that only dynamic testing can detect. Notably, the OpenAI team found that “repeated sampling” significantly improved outcomes: by generating multiple solutions and running each against the tests, they could pick a correct one with 100 attempts per problem, the success rate rose to 70.2\% on the benchmark \cite{evaluatingChatgpt}. This approach is essentially brute-force coupled with dynamic testing: the model produces many candidates, and automated dynamic tests filter out the incorrect ones, leaving (hopefully) at least one correct solution. Such results underscore how fundamental dynamic testing is when dealing with AI-generated code. Without running the code and checking its behavior, there’s no reliable way to know if an AI-written program is correct. Functional tests, runtime checks, and security analyses must be applied just as they would for human-written code if not more rigorously, given that AI suggestions might introduce non-obvious errors.
    \item \textbf{Formal Verification and Symbolic Execution} Formal verification refers to mathematically proving that code meets certain specifications or is free of certain categories of bugs. This is typically done with tools like model checkers, theorem provers, or by using symbolic execution engines that explore many possible execution paths of the code. While formal methods have historically been niche (used for safety-critical or security-critical software), they are gaining attention in the context of AI-generated code as a way to rigorously vet the correctness and security of machine-written programs. One notable example is a whitepaper by CSET (Georgetown University) that evaluated AI-generated code using a bounded model checker. The researchers used ESBMC, a formal verification tool, to check code generated by five different AI models against certain correctness and safety properties.  They treated a “verification failed” result as an indicator of insecure or buggy code. The findings were sobering: “Overall, we saw a high rate of unsuccessful verification across the five models… 48\% of each model’s code samples resulted in bugs that could be detected by ESBMC (verification failed)” \cite{cybersecurity}. Another academic effort is the ACCA framework (Automating the Correctness Assessment of AI-generated Code) by Cotroneo et al., 2024. While its focus is on functional correctness of AI-generated code, it uses symbolic execution which inherently can catch certain security issues (e.g., if the code would crash or mis-handle certain inputs). ACCA was evaluated on AI-generated assembly code for security tasks, comparing outputs to a reference implementation using symbolic execution \cite{cotroneo2024acca}
    \item \textbf{Organizational Policies and AI-Specific Governance} While not a “technical” verification method per se, the policies and processes an organization implements around AI-generated code significantly affect security outcomes. Recognizing that AI-generated code can introduce risk, many companies are instituting guidelines to govern its use and ensure verification practices are followed. Industry surveys paint a picture of organizations grappling with this: A 2024 Venafi report found that 63\% of security leaders felt they lack visibility into where AI is used in development, and fewer than half of companies had policies to ensure safe use of AI in coding \cite{CyberLeaders}
    Forward-looking organizations are addressing this by establishing rules such as:
    \begin{itemize}
        \item Requiring code reviews for AI contributions
        \item Defining acceptable use cases of AI-generated code
        \item Training and educating developers on the risks of AI-generated code
        \item Monitoring and logging the usage of AI-generated code
    \end{itemize}
\end{enumerate}


\subsection{Research Gaps}
Despite existing research providing insights into productivity enhancements and security vulnerabilities associated with AI-assisted coding tools, several critical gaps remain.

Firstly, current literature primarily addresses productivity improvements and security risks independently, with limited exploration into the inherent trade-offs developers face between coding speed and security when leveraging AI. There is insufficient research on developer perceptions and decision-making processes regarding these trade-offs, particularly how developers balance the immediate productivity benefits against the potential introduction of security vulnerabilities.

Secondly, the effectiveness of existing security verification practices specifically tailored to AI-generated code has not been adequately investigated. While traditional verification methods are well-established for manually-written code, their effectiveness and adaptability for assessing AI-generated code remain unclear. Given the unique vulnerabilities introduced by AI coding assistants, research into specialized verification and validation methodologies is crucial.

Recent works such as DeVAIC \cite{deluca2024devaic} and ACCA \cite{cotroneo2024acca} propose security-specific static analysis and symbolic execution frameworks targeted at AI-generated code, achieving up to 94\% accuracy in detecting vulnerabilities. However, adoption in real-world development pipelines remains minimal.

Furthermore, benchmarking studies like CodeSecEval have evaluated AI models' ability to generate secure code and repair vulnerabilities, revealing persistent challenges in model reliability \cite{wang2024codesec}. These findings reinforce the notion that existing tools are not yet sufficient for automated security assurance.

From an industry perspective, multiple reports emphasize a growing concern. A 2024 joint survey by Venafi and CyberArk found that 66\% of security leaders believe their teams cannot keep up with the speed at which AI-generated code is produced \cite{venafi2024orgs}. SCWorld echoes this concern, suggesting that AI’s coding speed outpaces the ability of traditional security reviews to catch vulnerabilities \cite{scworld2024ai}. Moreover, a CyberArk press release reported that 83\% of organizations use AI to generate code despite mounting security concerns \cite{cyberark2024press}.

Addressing these gaps will enable the development of comprehensive guidelines and security frameworks, supporting developers in effectively utilizing AI tools without compromising application security.
