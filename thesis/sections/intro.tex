\section{Introduction}

While AI-assisted coding tools such as GitHub Copilot and ChatGPT have demonstrated productivity benefits \cite{peng2023impact, weisz2025examining}, security concerns remain a significant challenge \cite{fu2025security, perry2023}. Existing research has explored different aspects of AI-generated code, including productivity impact \cite{peng2023impact, weisz2025examining}, security risks \cite{fu2025security, perry2023}, and how developers interact with these tools \cite{ranim2024}. However, a major gap in the literature lies in understanding how developers balance efficiency and security concerns when using AI-assisted coding tools, as well as evaluating the effectiveness of security verification practices in mitigating vulnerabilities.

Several studies have investigated the security weaknesses of AI-generated code. Fu et al. \cite{fu2025security} and Perry et al. \cite{perry2023} found that AI-generated code often introduces common vulnerabilities such as SQL injection and improper authentication, while Asare et al. \cite{asare2024} examined how AI-generated code compares to human-written code in terms of security flaws.  Meanwhile, Peng and Weisz \cite{peng2023impact, weisz2025examining} have highlighted the productivity gains associated with AI-assisted coding, showing that developers can complete tasks more efficiently with these tools. Despite these insights, existing research largely treats security and productivity as separate concerns, without examining how developers perceive or navigate the trade-offs between them.

Additionally, recent research has assessed how developers interact with AI tools. The observational study by Khojah et al. \cite{ranim2024} found that developers often use AI assistants for guidance rather than copying code verbatim, suggesting that AI tools influence coding decisions beyond just code generation. Similarly, the study by Perry et al. \cite{perry2023} investigated whether AI usage leads to more insecure code but did not explore developer awareness of these risks or their strategies for mitigation.

Human review remains a fundamental line of defense for all code, including AI-generated snippets. However, with the rapid pace at which AI tools can generate code, is manual verification still a practical and efficient approach? Does the speed of AI-generated output outpace the ability of security teams to audit and validate it effectively? Or have other verification tools and practices been developed that can match the speed of code generation while maintaining the security and reliability of human review?

Our study aims to bridge these gaps by addressing the following research questions:
\begin{itemize}
	\item \textbf{RQ1}: How do developers perceive the trade-off between speed in writing code and security when using AI tools?
	\item \textbf{RQ2:} What are the existing security verification practices for AI-generated code in web applications?
  	\begin{itemize}
    	\item \textbf{RQ2.1:} How effective are the existing security verification practices for AI-generated code in web applications?
  	\end{itemize}
\end{itemize}

By combining developer surveys and literature review with a security analysis of AI-generated code, this research seeks to provide a more comprehensive understanding of how developers balance efficiency and security. Additionally, it evaluates whether current security verification methods are sufficient for detecting vulnerabilities in AI-generated code, offering insights into potential improvements for secure AI-assisted software development.
Study conclusion [bla bla bla]