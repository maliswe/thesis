\section{Introduction}
The adoption of AI-assisted programming tools, such as GitHub Copilot and ChatGPT-based code generation, has reshaped software development workflows. These tools promise increased developer productivity \cite{microsoft2023, ibm2025} while raising concerns about security vulnerabilities in AI-generated code \cite{fu2023, perry2023}.

Studies indicate that AI-assisted developers complete coding tasks significantly faster than those without AI assistance \cite{microsoft2023}. IBMâ€™s internal study of AI pair programming tools found that while most developers reported efficiency gains, factors such as task type and experience level influenced the actual benefits \cite{ibm2025}. However, industry reports challenge these findings, with Uplevel Data Labs showing that Copilot users introduced more bugs without an overall increase in productivity \cite{uplevel2024}.

Security evaluations have revealed that a substantial portion of AI-generated code contains vulnerabilities \cite{fu2023, perry2023}. Perry et al. found that AI-assisted developers unknowingly wrote insecure code while maintaining high confidence in its correctness \cite{perry2023}. Similar concerns were raised by Fu et al., who analyzed real-world AI-generated code and found vulnerability rates exceeding 30\% \cite{fu2023}.

Given these concerns, this study aims to evaluate the trade-offs between productivity and security in AI-assisted development. The research will combine a developer survey \cite{developer-adoption, snyk2023} with security analysis of AI-generated code \cite{fu2023, asare2024}. The findings will provide guidance for balancing efficiency and security in AI-driven coding workflows.
