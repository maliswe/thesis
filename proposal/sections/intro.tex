\section{Introduction}

Generative AI has revolutionized the software development workflow by automating repetitive coding tasks, suggesting solutions, and improving code quality. Tools such as GitHub Copilot and OpenAI's ChatGPT provide real-time code suggestions and debugging assistance, enabling developers to work faster and more efficiently \cite{tools}. Research has shown that AI-powered coding assistants significantly improve programmer productivity by reducing development time and minimizing errors \cite{ferdiana2024}. These tools are especially beneficial for early career developers, helping them adapt to industry challenges by lowering barriers to entry and increasing their productivity \cite{ferdiana2024}. This study focuses on how these tools have reshaped software development practices and workflows. 

Furthermore, while AI-driven coding tools improve productivity, they also introduce potential security vulnerabilities that can compromise the integrity and reliability of software \cite{security-vulnerabilities}. Studies have shown that AI-generated code can contain security flaws that developers may not immediately recognize, increasing the risk of exploitation \cite{snyk2023}. These risks include the generation of insecure code, exposure to dependency vulnerabilities, and the inclusion of hard-coded credentials. According to a recent report by Snyk, more than 50\% companies using AI-generated code encountered security issues, underscoring the need for rigorous validation and security assessments \cite{snyk2023}. 

Additionally, AI models trained on public code repositories may unintentionally replicate vulnerabilities found in their training data, which can increase security risks \cite{ai-training-risks}. It is essential to understand these security implications to ensure that the benefits of AI tools are used effectively while minimising potential risks.


