\section{Introduction}

While AI-assisted coding tools such as GitHub Copilot and ChatGPT have demonstrated productivity benefits \cite{peng2023impact, weisz2025examining}, security concerns remain a significant challenge \cite{fu2025security, perry2023}. Existing research has explored different aspects of AI-generated code, including productivity impact \cite{peng2023impact, weisz2025examining}, security risks \cite{fu2025security, perry2023}, and how developers interact with these tools \cite{ranim2024}. However, a major gap in the literature lies in understanding how developers balance efficiency and security concerns when using AI-assisted coding tools, as well as evaluating the effectiveness of security verification practices in mitigating vulnerabilities.

Several studies have investigated the security weaknesses of AI-generated code. Fu et al. \cite{fu2025security} and Perry et al. \cite{perry2023} found that AI-generated code often introduces common vulnerabilities such as SQL injection and improper authentication, while Asare et al. \cite{asare2024} examined how AI-generated code compares to human-written code in terms of security flaws. Additionally, research by Peng and Weisz \cite{peng2023impact, weisz2025examining} has highlighted the productivity gains associated with AI-assisted coding, suggesting that developers complete tasks more efficiently when using these tools. However, while these studies provide valuable insights, they largely treat security risks and productivity benefits as separate issues rather than examining how developers perceive and navigate the trade-offs between them.

Furthermore, recent research has assessed how developers interact with AI tools. The observational study by Khojah et al. \cite{ranim2024} found that developers often use AI assistants for guidance rather than copying code verbatim, suggesting that AI tools influence coding decisions beyond just code generation. Similarly, the study by Perry et al. \cite{perry2023} investigated whether developers write more insecure code when using AI, but it did not explore whether developers are aware of these risks or how they attempt to mitigate them. Additionally, while prior studies have analyzed security vulnerabilities in AI-generated code \cite{fu2025security, perry2023, asare2024}, there is limited research evaluating the effectiveness of security verification methods for AI-assisted development.

Our study aims to bridge these gaps by addressing the following research questions:
\begin{itemize}
    \item \textbf{RQ1}: How do developers perceive the trade-off between speed in writing code and security when using AI tools? (Perception)
    \item \textbf{RQ2}: How effective are existing security verification practices for AI-generated code in web applications?
\end{itemize}

By combining developer surveys with security analysis of AI-generated code, this research will provide a more comprehensive understanding of how developers navigate the balance between efficiency and security. Additionally, it will evaluate whether existing security verification methods are adequate for detecting vulnerabilities in AI-generated code, offering insights into potential improvements in secure AI-assisted software development.
